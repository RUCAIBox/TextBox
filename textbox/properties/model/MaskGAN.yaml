train_batch_size: 128
eval_batch_size: 128
embedding_size: 128 # 650 for imdb
gen_share_embedding: True
dis_share_embedding: True
hidden_size: 128 # 650 for imdb
num_enc_layers: 2
bidirectional: False
context_size: 128 # 650 for imdb
num_dec_layers: 2
dropout_ratio: 0.5
rnn_type: "lstm"
attention_type: "LuongAttention"
alignment_method: "concat"
mask_strategy: "random"
is_present_rate: 0.5
advantage_clipping: 5
rl_discount_rate: 0.8835659
user_token_list: ["<|mask|>"]

# GAN training settings
gen_learning_rate: 2e-4
dis_learning_rate: 5e-6
critic_learning_rate: 5e-4
pretrain_lm_epochs: 1
g_mask_pretraining_epochs: 1 # 4 for  imdb
d_pretraining_epochs: 1
adversarail_training_epochs: 10
grad_clip: 10.0
